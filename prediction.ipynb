{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart Attack analysis and prediction \n",
    "\n",
    "\n",
    "Project Content\n",
    "Introduction\n",
    "1.1 Examining the Project Topic\n",
    "1.2 Recognizing Variables In Dataset\n",
    "First Organization\n",
    "2.1 Required Python Libraries\n",
    "2.1.1 Basic Libraries\n",
    "2.2 Loading The Dataset\n",
    "2.3 Initial analysis on the dataset\n",
    "2.3.1 Analysis Outputs(1)\n",
    "Preparation for Exploratory Data Analysis(EDA)\n",
    "3.1 Examining Missing Values\n",
    "3.2 Examining Unique Values\n",
    "3.2.1 Analysis Outputs(2)\n",
    "3.3 Separating variables (Numeric or Categorical)\n",
    "3.4 Examining Statistics of Variables\n",
    "3.4.1 Analysis Outputs(3)\n",
    "Exploratory Data Analysis(EDA)\n",
    "4.1 Uni-variate Analysis\n",
    "4.1.1 Numerical Variables(Analysis with Distplot)\n",
    "4.1.1.1 Analysis Outputs(4)\n",
    "4.1.2 Categorical ariables(Analysis with Pie Chart)\n",
    "4.1.2.1 Analysis Outputs(5)\n",
    "4.1.2.2 Examining the Missing Data According to the Analysis Result\n",
    "4.2 Bi-Variate Analysis\n",
    "4.2.1 Numerical Variables - Target Variable(Analysis with FacetGrid)\n",
    "4.2.1.1 Analysis Outputs(6)\n",
    "4.2.2 Categorical Variables-Target Variable(Analysis with Count Plot)\n",
    "4.2.2.1 Analysis Outputs(7)\n",
    "4.2.3 Examining Numeric Variables Among Themselves(Analysis with Pair Plot)\n",
    "4.2.3.1 Analysis Outputs(8)\n",
    "4.2.4 Feature Scaling with the RobustScaler Method\n",
    "4.2.5 Creating a New DataFrame with the Melt() Function\n",
    "4.2.6 Numerical - Categorical Variables (Analysis with Swarm Plot)\n",
    "4.2.6.1 Analysis Outputs(9)\n",
    "4.2.7 Numerical - Categorical Variables (Analysis with Box Plot)\n",
    "4.2.7.1 Analysis Outputs(10)\n",
    "4.2.8 Relationships between variables(Analysis with Heatmap)\n",
    "\n",
    "The medical name of heart attack is \"Myocardial infarction\".\n",
    "Heart attack in short It is the occlusion of the vessel by plaque-like lesions filled with cholesterol and fat.\n",
    "The lesion is called abnormal conditions that occur in the organs where the disease is located.\n",
    "As a result of the blockage, the blood flow is completely cut off and a heart attack that can lead to death occurs.\n",
    "\n",
    "\n",
    "The heart is a powerful pump that pumps blood throughout the body 60-80 times per minute at rest.\n",
    "While meeting the blood needs of the whole body, it also needs to be fed and taker blood.\n",
    "These vessels that feed the heart itself are called coronary arteries\n",
    "Coronary insufficiency occurs when there is a disruption in the circulation of the coronary arteries.\n",
    "The cases of coronary insufficiency vary according to the type, degree and location of the stenosis in the coronary vessels\n",
    "While some patients may have chest pain that occurs only during physical activity and is relieved by rest, sometimes a heart attack may occur as a result of sudden occlusion of the vessels starting with re\n",
    "chest pain and leading to sudden death\n",
    "\n",
    "\n",
    "1.2 Recognising Variables in Dataset\n",
    "add Codeadd Markdown\n",
    "Variable definitions in the Dataset\n",
    "\n",
    "Age: Age of the patient\n",
    "Sex: Sex of the patient\n",
    "exang: exercise induced angina (1 = yes; 0 = no)\n",
    "ca: number of major vessels (0-3)\n",
    "cp: Chest Pain type chest pain type\n",
    "Value 1: typical angina\n",
    "Value 2: atypical angina\n",
    "Value 3: non-anginal pain\n",
    "Value 4: asymptomatic\n",
    "trtbps: resting blood pressure (in mm Hg)\n",
    "chol: cholestoral in mg/dl fetched via BMI sensor\n",
    "fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "rest_ecg: resting electrocardiographic results\n",
    "Value 0: normal\n",
    "Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "thalach: maximum heart rate achieved\n",
    "target: 0= less chance of heart attack 1= more chance of heart attack\n",
    "\n",
    "Additional variable descriptions to help\n",
    "\n",
    "age - age in years\n",
    "\n",
    "sex - sex (1 = male; 0 = female)\n",
    "\n",
    "cp - chest pain type (1 = typical angina; 2 = atypical angina; 3 = non-anginal pain; 0 = asymptomatic)\n",
    "\n",
    "trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n",
    "\n",
    "chol - serum cholestoral in mg/dl\n",
    "\n",
    "fbs - fasting blood sugar > 120 mg/dl (1 = true; 0 = false)\n",
    "\n",
    "restecg - resting electrocardiographic results (1 = normal; 2 = having ST-T wave abnormality; 0 = hypertrophy)\n",
    "\n",
    "thalach - maximum heart rate achieved\n",
    "\n",
    "exang - exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "oldpeak - ST depression induced by exercise relative to rest\n",
    "\n",
    "slope - the slope of the peak exercise ST segment (2 = upsloping; 1 = flat; 0 = downsloping)\n",
    "\n",
    "ca - number of major vessels (0-3) colored by flourosopy\n",
    "\n",
    "thal - 2 = normal; 1 = fixed defect; 3 = reversable defect\n",
    "\n",
    "num - the predicted attribute - diagnosis of heart disease (angiographic disease status) (Value 0 = < diameter narrowing; Value 1 = > 50% diameter narrowing)\n",
    "z\n",
    "\n",
    "First Organization \n",
    "\n",
    "\n",
    "\n",
    "2.1 Required Python Libraries \n",
    "\n",
    "2.1.1 Basic Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Loading The Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Initial analysis on the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\"age\", \"sex\", \"cp\", \"trtbps\", \"chol\", \"fbs\", \"rest_ecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = new_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 Analysis Outputs(1) \n",
    "The Data Set consists of 303 Rows and 14 Columns.\n",
    "The type of all the variables in the data set are in numerical format. (Integer Or Float)\n",
    "According to first impressions, there is no missing value(NaN Value) in the data set.\n",
    "\n",
    "3. Preparation for Exploratory Data Analysis(EDA) \n",
    "\n",
    "\n",
    "3.1 Examining Missing Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnull_number = []\n",
    "for i in df.columns:\n",
    "    x = df[i].isnull().sum()\n",
    "    isnull_number.append(x)\n",
    "    \n",
    "pd.DataFrame(isnull_number, index = df.columns, columns = [\"Total Missing Values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno\n",
    "missingno.bar(df, color = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Examining Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cp\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cp\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_number = []\n",
    "for i in df.columns:\n",
    "    x = df[i].value_counts().count()\n",
    "    unique_number.append(x)\n",
    "    \n",
    "pd.DataFrame(unique_number, index = df.columns, columns = [\"Total Unique Values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis Outputs(2) \n",
    "According to the result from the unique value dataframe;\n",
    "We determined the variables with few unique values ​​as categorical variables, and the variables with high unique values ​​as numeric variables.\n",
    "In this context, Numeric Variables: “age”, “trtbps”, “chol”, “thalach” and “oldpeak ”\n",
    "Categorical Variables: \"sex\", \"cp\", \"fbs\", \"rest_ecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "In the next section, we will separate these 2 groups into 2 different lists.\n",
    "\n",
    "3.3 Separating variables (Numeric or Categorical) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var = [\"age\", \"trtbps\", \"chol\", \"thalach\", \"oldpeak\"]\n",
    "categoric_var = [\"sex\", \"cp\", \"fbs\", \"rest_ecg\", \"exang\", \"slope\", \"ca\", \"thal\", \"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Examining Statistics of Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_var].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4.1 Analysis Outputs(3) \n",
    "Note: Different graphics were used in the analysis to develop visualization skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"age\" variable\n",
    "sns.distplot(df[\"age\"], hist_kws = dict(linewidth = 1, edgecolor = \"k\"));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of \"age\" variable according to Describe() method¶\n",
    "The minimum value of the ages is 29, and the maximum value is 77.\n",
    "So, if we don't look at other data, only these two data should mean that the midpoint must be 53 from the mathematical operation ((29 + 77) / 2).\n",
    "The mean of the data for the age is 54. Isn't the average of the minimum and maximum values that we found just by mathematical calculations 53?\n",
    "They are almost equal to each other.\n",
    "That means the age variable has a normal distribution. The normal distribution is the ideal statistical distribution for us.\n",
    "Let's look at the quartiles.\n",
    "The data average is in the middle of the 25% and 75% quarters. This means that the age variable is prone to the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"trtbps\" variable\n",
    "sns.distplot(df[\"trtbps\"], hist_kws = dict(linewidth = 1, edgecolor = \"k\"), bins = 20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of \"trtbps\" variable according to Describe() method\n",
    "The minimum value for the \"trtbps\" variable is 94, and the maximum value is 200. The average of these two numbers is 147.\n",
    "The major average is 131.\n",
    "There is not much difference between 131 and 147. \"trtbps\" data tends to be normally distributed.\n",
    "However, very little data remains on the left side of 147. This means that when we take the midpoint of the minimum value and the maximum value as a basis, the data on the left is more than on the right.\n",
    "So, there will be a small queue towards the right side due to the lack of data on the right side.\n",
    "In summary, we can say that this data is prone to a normal distribution, but there is a slight right skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"chol\" variable\n",
    "sns.distplot(df[\"chol\"], hist = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of \"chol\" variable according to Describe() method\n",
    "The minimum value for the \"chol\" variable is 126, and the maximum is 564. The middle of these two values is 345.\n",
    "The major average is 246.\n",
    "When viewed, the mean is slight to the left of the midpoint of the minimum and maximum value.\n",
    "Let's check the quartiles. There are values up to 274 in the first 75 percent. Considering that the maximum value is 564, we see that most of the data is on the left.\n",
    "So data is slightly skewed to the right due to values that outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"thalach\" variable\n",
    "x, y = plt.subplots(figsize = (8, 6))\n",
    "sns.distplot(df[\"thalach\"], hist = False, ax = y)\n",
    "y.axvline(df[\"thalach\"].mean(), color = \"r\", ls = \"--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of \"thalach\" variable according to Describe() method\n",
    "The minimum value of the \"thalach\" variable is 71. The maximum value is 202. According to these two values, the midpoint value is 137.\n",
    "The central average is 149.6\n",
    "So there is a left skew, although not much.\n",
    "When we examine the quartiles, The 25%, 50%, and 75% sections between the minimum and maximum values are homogeneously distributed.\n",
    "As a result, the data tends to be normally distributed, but there is a slight left skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"oldpeak\" variable\n",
    "x, y = plt.subplots(figsize = (8, 6))\n",
    "sns.distplot(df[\"oldpeak\"], hist_kws = dict(linewidth = 1, edgecolor = \"k\"), bins = 20, ax = y)\n",
    "y.axvline(df[\"oldpeak\"].mean(), color = \"r\", ls = \"--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of \"oldpeak\" variable according to Describe() method\n",
    "The minimum value of the \"oldpeak\" variable is 0, the maximum value is 6.20, so the middle point according to these two values is 3.10\n",
    "When we look at the mean, we see that it is 1.03\n",
    "When we examine the quartiles, 75% of the data consists of values up to 1.60\n",
    "This shows that; There is an incredible right skew in the data.\n",
    "\n",
    "4. Exploratory Data Analysis(EDA) \n",
    "\n",
    "\n",
    "4.1 Uni-variate Analysis \n",
    "\n",
    "4.1.1 Numerical Variables(Analysis with Distplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_axis_name = [\"Age of the Patient\", \"Resting Blood Pressure\", \"Cholesterol\", \"Maximum Heart Rate Achieved\", \"ST Depression\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font = {\"family\" : \"arial\", \"color\" : \"darkred\", \"weight\" : \"bold\", \"size\" : 15}\n",
    "axis_font = {\"family\" : \"arial\", \"color\" : \"darkblue\", \"weight\" : \"bold\", \"size\" : 13}\n",
    "\n",
    "for i, z in list(zip(numeric_var, numeric_axis_name)):\n",
    "    plt.figure(figsize = (8, 6), dpi = 80)\n",
    "    sns.distplot(df[i], hist_kws = dict(linewidth = 1, edgecolor = \"k\"), bins = 20)\n",
    "    \n",
    "    plt.title(i, fontdict = title_font)\n",
    "    plt.xlabel(z, fontdict = axis_font)\n",
    "    plt.ylabel(\"Density\", fontdict = axis_font)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.1.1 Analysis Outputs(4) \n",
    "Age Variable\n",
    "The vast majority of patients are between 50 and 60.\n",
    "There is a remarkable place on the chart. There is a decrease in patients between the ages of 47-and 50.\n",
    "It looks like there are no outliers in the variable.\n",
    "Trtbps Variable\n",
    "The resting blood pressure of most patients is generally between 110 and 140.\n",
    "Values after 180 can be considered as outliers.\n",
    "There is hefty patient traffic between 115-120, 125-130, and 155-160 values.\n",
    "Cholesterol Variable\n",
    "Cholesterol value in most patients is between 200-and 280.\n",
    "Values after 380 can be considered as outliers.\n",
    "Thalach Variable\n",
    "The maximum heart rate achieved in most patients is between 145-and 170.\n",
    "In particular, The values before 80 can be considered outliers.\n",
    "Oldpeak Variable\n",
    "Values of the vast majority of patients in the variable range from 0 to 1.5.\n",
    "Especially values after 2.5 can be considered as outliers.\n",
    "\n",
    "4.1.2 Categorical Variables(Analysis with Pie Chart) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_axis_name = [\"Gender\", \"Chest Pain Type\", \"Fasting Blood sugar\", \"Resting Electrocardiographic Results\",\n",
    "                      \"Exercise Induced Angina\", \"The Slope of ST Segment\", \"Number of Major Vessels\", \"Thal\", \"Target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(categoric_var, categoric_axis_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cp\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df[\"cp\"].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font = {\"family\" : \"arial\", \"color\" : \"darkred\", \"weight\" : \"bold\", \"size\" : 15}\n",
    "axis_font = {\"family\" : \"arial\", \"color\" : \"darkblue\", \"weight\" : \"bold\", \"size\" : 13}\n",
    "\n",
    "for i, z in list(zip(categoric_var, categoric_axis_name)):\n",
    "    fig, ax = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    observation_values = list(df[i].value_counts().index)\n",
    "    total_observation_values = list(df[i].value_counts())\n",
    "    \n",
    "    ax.pie(total_observation_values, labels= observation_values, autopct = '%1.1f%%', startangle = 110, labeldistance = 1.1)\n",
    "    ax.axis(\"equal\") # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    \n",
    "    plt.title((i + \"(\" + z + \")\"), fontdict = title_font) # Naming Pie Chart Titles\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1.2.1 Analysis Outputs(5) \n",
    "Sex Variable\n",
    "68.3% of the patients are male, 31.7% are female.\n",
    "So, the number of male patients is more than twice that of female patients.\n",
    "Cp Variable\n",
    "Almost half of the patients have an observation value of 0. In other words, there is asymptomatic angina\n",
    "Half of the patients are asymptomatic; they have pain without symptoms.\n",
    "If we examine the other half of the pie chart, 1 out of 4 patients has an observation value of 2.\n",
    "In other words, atypical angina is in 29% of the patients.\n",
    "This observation value shows patients with shortness of breath or non-classical pain.\n",
    "The other two observation values are less than the others.\n",
    "16.5% of patients have a value of 1. In other words, typical angina is seen. Typical angina is the classic exertion pain that comes during any physical activity.\n",
    "The other 8% has the value of non-anginal pain, which is three types of angina.\n",
    "Non-anginal pain is the term used to describe chest pain that is not caused by heart disease or a heart attack.\n",
    "Fbs Variable\n",
    "The vast majority of patients have an observation value of 1. In other words, 85%.\n",
    "The fasting blood sugar of these patients is more than 120 mg/dl.\n",
    "The remaining 15 percent have a less than 120 mg/dl fasting blood glucose level.\n",
    "Rest_ecg Variable\n",
    "The thing that draws attention to the image of this variable is that the number of patients with two observation values is negligible.\n",
    "It has a value of 1.3 percent. When we look at all of these patients, it is not a very important number.\n",
    "This value represents the ST and T wavelengths of the patients.\n",
    "Another point that draws attention to this graph is; The total numbers of other patients with observation values of 1 and 0 are almost equal.\n",
    "The size of those with 1, that is, the orange part on the graph is 50.2%\n",
    "This means that the resting electrocardiographic results of these patients are normal.\n",
    "The percentage of patients with a value of 0 is 48.5%.\n",
    "That is, the patients' values of 48.5% are normal.\n",
    "Exang Variable\n",
    "We have said that this variable stands for exercise-induced angina.\n",
    "Angina is the chest pain caused by the coronary artery's involuntary contraction that feeds the heart.\n",
    "According to the variable \"exang,\" the pain caused by this angina is represented by a value of 1 if it occurs with any exercise and 0 if it does not.\n",
    "In this context, Values 0 are more than twice as values 1. More than half of the patients do not have exercise-induced angina.\n",
    "Slope Variable\n",
    "The minimum observation value is 0 with 7 percent.\n",
    "This is patients with a downward slope of the ST wavelength.\n",
    "The other two observation values are almost equal to each other.\n",
    "The ST wavelength of half of the remaining patients is 1, that is straight, while the observation value of the other half is 2, that is, the ST wavelength is sloped upwards.\n",
    "Ca variable\n",
    "This variable is the number of great vessels colored by fluoroscopy.\n",
    "In more than half of the patients, 57.8 percent, the number of large vessels is 0. That is, the number of large vessels colored by fluoroscopy is absent.\n",
    "After 0 observation value, the other value with the most slices in the pie chart 1\n",
    "The number of large vessels observed in 21.5% of the patients is 1\n",
    "The majority of patients have an occlusion in their veins. Therefore, large vessels cannot be observed with the fluoroscopy technique.\n",
    "Thal Variable\n",
    "The \"Thal\" variable is short for the \"Thallium stress test.\"\n",
    "The thallium stress test is simply an imaging method that evaluates the amount of blood reaching the heart muscle and determines whether a person has coronary artery disease.\n",
    "There are three observation values in the description of this variable. However, the pie chart shows four values. Values 0, 1, 2 and 3.\n",
    "According to our research, the observation value of 0 is null. Therefore, in the next step, 0 observation values will be returned to null and filled with logical data.\n",
    "In this context, according to the thallium stress test results, 54.8 percent of the patients have two observation values, so the test result appears to be expected.\n",
    "36.8 percent has a value of 3, so we can say that this value is a reversible defect as an explanation.\n",
    "5.9 percent of patients have a value of 1, so the test result for these patients is a fixed defect.\n",
    "Target Variable\n",
    "More than half of the patients, 54.5 percent, have a heart attack risk. The remaining 45.5 percent have no heart attack risk.\n",
    "\n",
    "4.1.2.2 Examining the Missing Data According to the Analysis Result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"thal\"] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"thal\"] = df[\"thal\"].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[48, 281], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnull_number = []\n",
    "for i in df.columns:\n",
    "    x = df[i].isnull().sum()\n",
    "    isnull_number.append(x)\n",
    "    \n",
    "pd.DataFrame(isnull_number, index = df.columns, columns = [\"Total Missing Values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"thal\"].fillna(2, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[48, 281], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[[48, 281], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isnull_number = []\n",
    "for i in df.columns:\n",
    "    x = df[i].isnull().sum()\n",
    "    isnull_number.append(x)\n",
    "    \n",
    "pd.DataFrame(isnull_number, index = df.columns, columns = [\"Total Missing Values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"thal\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Bi-variate Analysis \n",
    "\n",
    "4.2.1 Numerical Variables - Target Variable(Analysis with FaceGrid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var.append(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font = {\"family\" : \"arial\", \"color\" : \"darkred\", \"weight\" : \"bold\", \"size\" : 15}\n",
    "axis_font = {\"family\" : \"arial\", \"color\" : \"darkblue\", \"weight\" : \"bold\", \"size\" : 13}\n",
    "\n",
    "for i, z in list(zip(numeric_var, numeric_axis_name)):\n",
    "    graph = sns.FacetGrid(df[numeric_var], hue = \"target\", height = 5, xlim = ((df[i].min() - 10), (df[i].max() + 10)))\n",
    "    graph.map(sns.kdeplot, i, shade = True)\n",
    "    graph.add_legend()\n",
    "    \n",
    "    plt.title(i, fontdict = title_font)\n",
    "    plt.xlabel(z, fontdict = axis_font)\n",
    "    plt.ylabel(\"Density\", fontdict = axis_font)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_var].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_var].corr().iloc[:, [-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1.1 Analysis Outputs(6) \n",
    "Note :\n",
    "The KDE plot shows the density of the feature for each value of the target variable. There are usually two-colored humps representing the two values of the Target variable. If the humps are well-separated and non-overlapping, there is a correlation with the Target. If the humps overlap a lot, that means the feature is not well-correlated with the Target because the Target is equally as common on those values of the feature.\n",
    "Age - Target Variable\n",
    "There is some interesting data in this chart. Typically, the general belief is that heart attack increases with age.\n",
    "However, the graph tells us that we are wrong about this.\n",
    "If you have noticed, the number of people at risk of heart attack decreases as age progresses.\n",
    "Let's take 55 as a base. There is an incredible increase in the blue graph after 55. In other words, there is a decrease in heart attack risk after the age of 55.\n",
    "On the contrary, a decrease occurs after 55 in the orange graph. In other words, there is a decrease in the risk of heart attack after the age of 55.\n",
    "To summarize, It's just a general thought that older people may be more likely to have a heart attack, but from a graph of the age and target distribution, it's clear that this isn't the case.\n",
    "And correlation between them -0.225439. There is a negative correlation, which we can call little.\n",
    "Trtbps - Target Variable\n",
    "It is complicated to predict whether a patient will have a heart attack from resting blood pressure data.\n",
    "Because if you notice, the two graphs are almost identical.\n",
    "For example, according to the \"trtbps\" value of two possibilities, 130 is the maximum point.\n",
    "Also, the Bumps on the graph overlap too much.\n",
    "This means that the feature does not correlate well with the Target variable. So, in summary, A patient with a \"trtbps\" value of 130 may or may not be likely to have a heart attack.\n",
    "And according to the graph, we can say that there is little correlation between the two variables. Correlation between them -0.144931. There is a negative correlation, which we can call little.\n",
    "Chol - Target Variable\n",
    "In the graph, we can say that a cholesterol value of 200-250 is precarious for the patients.\n",
    "If you have noticed, especially at the beginning of the graph, patients with and without heart attack risk a similar image, while the increase in the probability of having a heart attack from 180 to 250 is noticeable.\n",
    "After the value of 250, it becomes difficult to differentiate the probability of patients having a heart attack.\n",
    "The correlation between the two variables is -0.085239. In other words, we can say that it is negative, but very little.\n",
    "Thalach - Target Variable\n",
    "There is a situation that is evident in this graph.\n",
    "The higher the maximum reached heart rate, the higher the probability of the patient having a heart attack.\n",
    "If you have noticed, there is a parallel increase in the two possibilities up to 150. However, after a value of 150, patients with a low probability of having a heart attack decrease, while patients with a high-risk probability increase incredibly.\n",
    "It is evident in the distinction between the two bumps in the graph. This indicates that there is a correlation between the two variables.\n",
    "The correlation between the two variables is 0.421741. In other words, we can say that it is positive and moderate.\n",
    "Oldpeak - Target Variable\n",
    "When we look at the graph, if the value of this variable is between 0 and 1.5, there is a significant increase in the probability of having a heart attack.\n",
    "We can say that the range of 0 to 1.5 is the critical threshold for us.\n",
    "It is evident in the distinction between the two bumps in the graph. This indicates a correlation between the two variables.\n",
    "The correlation between the two variables is -0.430696. In other words, we can say that it is negative and moderate.\n",
    "\n",
    "4.2.2 Categorical Variables - Target Variable(Analysis with Count Plot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_font = {\"family\" : \"arial\", \"color\" : \"darkred\", \"weight\" : \"bold\", \"size\" : 15}\n",
    "axis_font = {\"family\" : \"arial\", \"color\" : \"darkblue\", \"weight\" : \"bold\", \"size\" : 13}\n",
    "\n",
    "for i, z in list(zip(categoric_var, categoric_axis_name)):\n",
    "    plt.figure(figsize = (8, 5))\n",
    "    sns.countplot(i, data = df[categoric_var], hue = \"target\")\n",
    "    \n",
    "    plt.title(i + \" - target\", fontdict = title_font)\n",
    "    plt.xlabel(z, fontdict = axis_font)\n",
    "    plt.ylabel(\"Target\", fontdict = axis_font)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categoric_var].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[categoric_var].corr().iloc[:, [-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2.1 Analysis Outputs(7) \n",
    "Sex - Target Variable\n",
    "Patients at high risk of heart attack from women are almost more than half of those with low.\n",
    "The situation is different for those with an observation value of 1, that is, for men. The blue-colored bar has more observation values.\n",
    "So men are more likely than not to have a heart attack.\n",
    "In summary, female patients are at higher risk for heart attack\n",
    "The correlation between the two variables is -0.280937. In other words, we can say that there is a negative low-intensity correlation.\n",
    "Cp - Target Variable\n",
    "First bar graph has an observation value of 0, asymptomatic pains. This pain was pain that occurred without symptoms.\n",
    "Patients with this pain are less likely to have a heart attack. When we look at the probability of not having a heart attack, we can say that it is almost three times the other.\n",
    "In the other three types of pain, having a heart attack is almost three times higher than the risk of not having it.\n",
    "In summary, If a patient's \"cp\" variable is 1, 2, or 3, we can say that the risk of heart attack is high.\n",
    "The correlation between the two variables is 0.433798. In other words, we can say that there is a positive, moderately strong correlation.\n",
    "Fbs - Target Variable\n",
    "The risk of heart attack is slightly higher in those with a 0 observation value as fasting blood sugar.\n",
    "Patients with fasting blood sugar lower than 120 mg/dl can be positioned as risky.\n",
    "If we look at the value of 1 observation value, the situation seems to be somewhat equal here.\n",
    "In other words, it is a little challenging to say anything definite about heart attack in patients with fasting blood sugar greater than 120 mg/dl.\n",
    "The correlation between the two variables is -0.028046. In other words, we can say that there is a very low-intensity correlation in the negative direction.\n",
    "Rest_ecg - Target Variable\n",
    "Attention should be paid to patients with a \"rest ecg\" value of 1. Because the risk of having a heart attack is almost two times higher than that of not having a heart attack.\n",
    "The other two observation values are also more likely not to have a heart attack.\n",
    "The correlation between the two variables is 0.137230. In other words, we can say that there is a positive low-intensity correlation.\n",
    "Exang - Target Variable\n",
    "Pain due to exercise does not affect the heart attack.\n",
    "If the patient's \"exang\" variable is 1, that is, if he has exercise-related angina, the probability of not having a heart attack is higher.\n",
    "In contrast, patients who do not have exercise-related angina are more likely to have a heart attack.\n",
    "This means that exercise-related pain has nothing to do with a heart attack.\n",
    "The correlation between the two variables is -0.436757. In other words, we can say that there is a negative, moderately strong correlation.\n",
    "Slope - Target Variable\n",
    "It is necessary to pay attention to the patients with a \"slope\" variable of 2.\n",
    "Because patients with an observation value of 2 are three times more likely to have a heart attack than not having a heart attack.\n",
    "If we examine other values, the risk of heart attack is lower in patients with an observation values of 0 and 1.\n",
    "Especially if the observation value is 1, if it has a flat slope, we can say that it is two times more likely not to have a heart attack.\n",
    "The correlation between the two variables is 0.345877. In other words, we can say that there is a positive, moderately strong correlation.\n",
    "Ca - Target variable\n",
    "The group at risk for this variable is patients with an observation value of 0.\n",
    "If you notice, the risk of heart attack is almost three times higher in patients with an observation value of 0.\n",
    "For 1, 2, and 3 observation values, this shows the opposite situation. In other words, patients with \"ca\" observation values of 1, 2, and 3 are almost twice as likely to have a heart attack as those not having them.\n",
    "There is something remarkable here. Although the number of patients with a \"CA\" value of 4 is less than the general population, the risk of having a heart attack seems to be higher.\n",
    "The correlation between the two variables is -0.363322. In other words, we can say that there is a negative, moderately strong correlation.\n",
    "Thal - Target Variable\n",
    "Patients with an observation value of 2 are three times more likely to have a heart attack than if they have not.\n",
    "The opposite is true for other values. The situation of not having a heart attack appears to be higher.\n",
    "The correlation between the two variables is -0.363322. In other words, we can say that there is a negative, moderately strong correlation.\n",
    "\n",
    "4.2.3 Examining Numeric Variables Among Themselves(Analysis with Pair Plot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numeric_var].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = sns.pairplot(df[numeric_var], diag_kind = \"kde\")\n",
    "graph.map_lower(sns.kdeplot, levels = 4, color = \".2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3.1 Analysis Outputs(8) \n",
    "Note :\n",
    "The pair plot allows us to see both distribution of single variables and relationships between two variables. Pair plots are a great method to identify trends for follow-up analysis.\n",
    "\n",
    "The scatter plot; is a drawing or mathematical diagram that typically uses Cartesian coordinates to display the values of two variables in a dataset. If the points are close to each other and have a regular appearance in terms of their directions, we can say a strong connection between the two variables. There is a directly proportional relationship between two variables in positive directional relationships. An increase in one variable causes an increase in the other variable. In negative relationships, there is an inversely proportional relationship between the variables. When the value of one variable decreases, the other variable also decreases.\n",
    "\n",
    "Age Variable\n",
    "The age variable with the most relationship is the \"thalach\" variable.\n",
    "There is a collection of points extending in the southwest direction.\n",
    "There is a negative correlation since the points are pointing downwards.\n",
    "However, since the points are somewhat scattered, this correlation is moderate.\n",
    "Trtbps Variable\n",
    "The relationship between \"Trtbps\" and other variables is weak.\n",
    "There is clutter in the graphics in general.\n",
    "With the age variable, the dots appear a little more often. There is a positive correlation.\n",
    "The lowest correlation is with the \"thalach\" variable. The dots are incredibly messy. There is no smooth distribution.\n",
    "Chol Variable\n",
    "The variables that have the highest correlation with the variable \"chol\" are \"age\" and \"trtbps\".\n",
    "However, the relationship between the \"age\" variable is slightly higher.\n",
    "So cholesterol increases with age.\n",
    "There is almost no relationship with other variables.\n",
    "Thalach Variable\n",
    "The age variable has the most relationship with the Thalach variable. There seems to be a moderately negative relationship.\n",
    "There is not much correlation between the variable \"trtbps\" and \"chol\". The dots are too scattered.\n",
    "When we look at the relationship with the \"old peak\" variable, we can say a close relationship with the middle level.\n",
    "Oldpeak Variable\n",
    "We cannot find an explicit expression when interpreting the graphics of this variable because the relationship with all variables is similar to each other.\n",
    "However, we can say this in general: There is a close relationship with moderate with all variables.\n",
    "\n",
    "4.2.4 Feature Scaling with the RobustScaler Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = robust_scaler.fit_transform(df[numeric_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(scaled_data, columns = numeric_var)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5 Creating a New DataFrame with the Melt() Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([df_scaled, df.loc[:, \"thal\"]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_data = pd.melt(df_new, id_vars = \"thal\", var_name = \"variables\", value_name = \"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 5))\n",
    "sns.swarmplot(x = \"variables\", y = \"value\", hue =\"thal\", data = melted_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.6 Numerical Variables - Categorical Variables (Analysis with Swarm Plot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_font = {\"family\" : \"arial\", \"color\" : \"black\", \"weight\" : \"bold\", \"size\" : 14}\n",
    "for i in df[categoric_var]:\n",
    "    df_new = pd.concat([df_scaled, df.loc[:, i]], axis = 1)\n",
    "    melted_data = pd.melt(df_new, id_vars = i, var_name = \"variables\", value_name = \"value\")\n",
    "    \n",
    "    plt.figure(figsize = (8, 5))\n",
    "    sns.swarmplot(x = \"variables\", y = \"value\", hue = i, data = melted_data)\n",
    "    \n",
    "    plt.xlabel(\"variables\", fontdict = axis_font)\n",
    "    plt.ylabel(\"value\", fontdict = axis_font)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.6.1 Analysis Outputs(9) \n",
    "Note:\n",
    "The swarm plot is a scatter plot used to represent categorical values. It is very similar to the strip plot, but it avoids overlapping points. It is not advisable to use this type of graph when the sample size is large.\n",
    "Sex - Numeric Variables\n",
    "Colors are intertwined between \"sex\" and other numerical variables in the graphics. It isn't easy to distinguish.\n",
    "In summary, the relationship between the categorical \"sex\" variable and the numerical variables is weak.\n",
    "Cp - Numeric Variables\n",
    "The variable that has the most relationship with the \"cp\" variable is the \"thalach\" variable. Each color appears in separate clusters, albeit slightly.\n",
    "In particular, individuals with an observation value of 0 appear distinctive.\n",
    "So chest pain has a moderate correlation with maximum attained heart rate.\n",
    "Observation units from other variables are intertwined. So there doesn't seem to be much correlation.\n",
    "Fbs - Numeric Variables\n",
    "Variables in orange in this graph have a small number. And this color is very scattered in the blue color.\n",
    "Therefore, no variable has a very high correlation between the \"Fbs\"\n",
    "Rest_ecg - Numeric Variables\n",
    "There is no strong relationship between the \"rest_ecg\" variable and the numeric variables due to the complexity of the colors in the chart.\n",
    "Exang - Numeric Variables\n",
    "The numerical variable that exercise-induced chest pain is most associated with is the \"thalach\" variable. Orange dots are clustered below, blue dots above.\n",
    "There is a better relationship between the \"exang\" and the old peak variables than the other. Of course, it is not as much as the \"thalach\" variable, but we can say that there is a moderate correlation.\n",
    "Colors look very messy in the graphs of other variables. Therefore, there is a low correlation between them.\n",
    "Slope - Numeric Variables\n",
    "The variables that the Slope variable has the most vital relationship with are the \"thalach\" and \"old peak\" variables. In these two variables, the colors can be observed separately.\n",
    "The same is not the case when we control the other three variables. The colors are very homogeneously distributed. It isn't straightforward to distinguish. We can say that there is a low correlation.\n",
    "Ca - Numeric Variables\n",
    "The variables with which the \"Ca\" variable is correlated are the \"age\", \"thalach,\" and \"old peak\" variables. A distinction can be seen between these variables.\n",
    "However, there is not a very strong correlation. We can say that it is below the middle level.\n",
    "Thal - Numeric Variables\n",
    "It seems to have a little more to do with \"old peak\". The same is valid with the \"thalach\" variable. The colors look a little more evenly clustered.\n",
    "Target - Numeric Variables\n",
    "The numerical variables with which the \"target\" variable has the most relationship are the \"thalach\" and \"old peak\" variables. Colors appear clustered.\n",
    "Because the colors are distributed homogeneously in other variables, there is not much relationship between them. We can say that there is a low-level correlation.\n",
    "\n",
    "4.2.7 Numerical Variables - Categorical Variables (Analysis with Box Plot) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_font = {\"family\" : \"arial\", \"color\" : \"black\", \"weight\" : \"bold\", \"size\" : 14}\n",
    "for i in df[categoric_var]:\n",
    "    df_new = pd.concat([df_scaled, df.loc[:, i]], axis = 1)\n",
    "    melted_data = pd.melt(df_new, id_vars = i, var_name = \"variables\", value_name = \"value\")\n",
    "    \n",
    "    plt.figure(figsize = (8, 5))\n",
    "    sns.boxplot(x = \"variables\", y = \"value\", hue = i, data = melted_data)\n",
    "    \n",
    "    plt.xlabel(\"variables\", fontdict = axis_font)\n",
    "    plt.ylabel(\"value\", fontdict = axis_font)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.7.1 Analysis Outputs(10) \n",
    "Sex - Numeric Variables\n",
    "There is no very high correlation between \"sex\" and numerical variables. There is a weak relationship with all of them.\n",
    "When the boxes of the observation values ​​of the gender variable are examined, it is seen that it is difficult to distinguish from each other.\n",
    "Cp - Numeric Variables\n",
    "There is a relationship between the \"cp\" and the \"thalach\" variables. The median of the blue box displays a shape outside of all the other boxes. * * In general, although there is not much difference between the Boxes, we can say that they have a higher relationship than the others.\n",
    "So, \"cp\", that is, chest pain, has a higher correlation with the maximum heart rate achieved.\n",
    "The relationship with other variables is weak.\n",
    "Fbs - Numeric Variables\n",
    "In general, it isn't easy to do between boxes. The median values ​​intersect with each other's boxes.\n",
    "Therefore, we can directly say a weak relationship between the \"fbs\" variable and the numerical variables.\n",
    "Rest_ecg - Numeric Variables\n",
    "There is a point to be noted here. When we look directly at the box plot graph, the \"slope\" variable seems to have a high correlation with the \"thalach\" and \"old peak\" variables. The green boxes stand apart from the others and are noticeable because of their separation.\n",
    "The green box represents patients with Hypertrophy. However, the number of these patients is deficient compared to others. In other words, we can say that it is a number that will not affect the main result when we consider the general.\n",
    "In this context, we should evaluate without considering the green boxes while making our evaluation here.\n",
    "When we interpret it that way, we see no strong relationship between the slope variable and other variables.\n",
    "Because the median values of the patients whose observation values are 0 and 1, that is, of the blue and orange boxes, cut each other's boxes. It is challenging to make a distinction.\n",
    "In summary, we can say that the relationship between the \"rest ecg\" variable and the numerical variables is weak.\n",
    "Exang - Numeric Variables\n",
    "There is a correlation between the \"Exang\" and \"Thalach\" variables. A clear distinction can be made between the boxes.\n",
    "They have a similar relationship with the \"old peak\" variable.\n",
    "There does not appear to be a relationship with the other three numerical variables.\n",
    "Slope - Numeric Variables\n",
    "There is a relationship between \"slope\" and \"old peak\". The difference between the boxes of the observation values is evident.\n",
    "We reached the same conclusion when we analyzed it with the \"swarm plot\". We said that there is a relationship between them. However, it did not appear so clearly. From here, we can be sure.\n",
    "It is difficult to understand the relationship between other variables by looking at this graph. Collaborative work can be done with the swarm plot.\n",
    "Ca - Numeric Variables\n",
    "In general, there is no strong correlation between the \"CA\" variable and the numerical variables.\n",
    "However, if we evaluate them among themselves, the age variable seems more related than the others. The separation of the observation values ​​from each other is visible.\n",
    "In the \"trtbps\" and \"chol\" variables, the medians are too much in each other. So there is not much of a distinction. We can say that the correlation between these two variables is low.\n",
    "Thal - Numeric Variables\n",
    "The variables that have the most relationship with the \"thal\" variable are the \"old peak\" and \"thalach\" variables. The median of the orange box stands outside the other two color boxes.\n",
    "This is not so common in other variables. The median values of other variables generally intersect with each other. That's why there isn't much of a relationship.\n",
    "Target - Numeric Variables\n",
    "In the \"old peak\" variable, the median value of the orange box goes outside the blue box. In other words, it shows that there is a more significant relationship between the \"old peak\" variable and the target compared to other numerical variables. We can say that there is a medium level of correlation.\n",
    "There is also a correlation between the \"thalach\" and the target variables. Again, the median of the orange box goes outside the blue box. This is an indication that there is a correlation compared to the others.\n",
    "When we examine the other three variables, there is not much correlation.\n",
    "\n",
    "4.2.8 Relationships between variables(Analysis with Heatmap) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2 = pd.concat([df_scaled, df[categoric_var]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 10))\n",
    "sns.heatmap(data = df_new2.corr(), cmap = \"Spectral\", annot = True, linewidths = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.8.1 Analysis Outputs(11) \n",
    "Age Variable\n",
    "The variable with the highest correlation with the \"age\" variable is the \"thalach\" variable. There is a negative correlation between them, which we can call moderately.\n",
    "The severity of the correlation is -0.40. In other words, there is an inverse relationship between the \"age\" and \"thalach\" variables.\n",
    "If we make a quick assessment in this context, we can say that the amount of heart rate reached decreases as age increases because there is an inverse proportion between them.\n",
    "Trtbps Variable\n",
    "The variable with the highest correlation with the \"trtbps\" variable is the \"age\" variable. The correlation between them is 0.28\n",
    "There is a positive low-intensity correlation.\n",
    "Chol Variable\n",
    "The variable with the highest correlation with the \"chol\" variable is the \"age\" variable\n",
    "There is a correlation with a magnitude of 0.21. This is a low positive correlation.\n",
    "So, we can say that as age increases, cholesterol also increases.\n",
    "Thalach Variable\n",
    "The variable with the highest correlation to the \"Thalach\" variable is the \"target\" variable.\n",
    "There is a 0.42 positive and moderate correlation between them. In other words, it is a variable that can directly trigger a heart attack.\n",
    "There is a variable with which this variable has many correlations.\n",
    "It means that the maximum heart rate reached maybe a situation triggered by other variables.\n",
    "Oldpeak Variable\n",
    "It has the most significant correlation ratio among this changing table. This correlation is -0.58 with the \"slope\" variable.\n",
    "There is a negative correlation between them, which is slightly above medium intensity.\n",
    "The most significant correlation after the \"slope\" variable is with the \"target\" variable.\n",
    "Sex Variable\n",
    "There is no robust correlation between the variable \"Sex\" and other variables.\n",
    "The highest figure is -0.28 with the target variable. There is a negative low-intensity correlation between them.\n",
    "Cp Variable\n",
    "Cp variable captures the high correlation with \"thalach\", \"exang\", and \"target\" variables.\n",
    "The highest is again the \"target\" variable. There is a direct proportion between them.\n",
    "Fbs Variable\n",
    "The \"Fbs\" variable generally does not correlate with other variables.\n",
    "The highest correlation with 0.18 belongs to the \"trtbps\" variable. There is a low positive correlation.\n",
    "But we can say that when fasting blood sugar increases, resting blood pressure also increases.\n",
    "Rest_ecg Variable\n",
    "There is no strong correlation between the \"Rest_ecg\" variable and other variables.\n",
    "The highest correlation was 0.14 with the \"target\" variable. There is a positive low-intensity correlation.\n",
    "Exang Variable\n",
    "The variable with the highest correlation to the exercise-induced angina variable is the target variable with -0.44\n",
    "Also, It seems to be associated with more than one variable except \"target\".\n",
    "Slope Variable\n",
    "The variable with the highest correlation to the \"slope\" variable is the old peak variable. There is an above-moderate correlation between these two. It is the most significant relationship in the table with 0.58\n",
    "It has a moderate correlation with the variables \"thalach\" and \"target\".\n",
    "The relationship with other variables is fragile\n",
    "Ca Variable\n",
    "The variable with which the \"Ca\" variable has the highest correlation is the target variable with -0.39.\n",
    "Then comes the \"age\" variable with 0.28. We can say that there is a low positive correlation with the age variable.\n",
    "Thal Variable\n",
    "The variable with which the \"Thal\" variable has the highest correlation is the variable \"target\" with -0.36.\n",
    "It has not had very high correlation coefficients with other variables.\n",
    "Target Variable\n",
    "The \"target\" variable correlates with more than one variable.\n",
    "In general, we can say a relationship is below the middle level.\n",
    "\n",
    "5. Preparation for Modeling \n",
    "\n",
    "\n",
    "\n",
    "5.1 Dropping Columns with Low Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"chol\", \"fbs\", \"rest_ecg\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Struggling Outliers \n",
    "\n",
    "5.2.1 Visualizing outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize = (20, 6))\n",
    "\n",
    "ax1.boxplot(df[\"age\"])\n",
    "ax1.set_title(\"age\")\n",
    "\n",
    "ax2.boxplot(df[\"trtbps\"])\n",
    "ax2.set_title(\"trtbps\")\n",
    "\n",
    "ax3.boxplot(df[\"thalach\"])\n",
    "ax3.set_title(\"thalach\")\n",
    "\n",
    "ax4.boxplot(df[\"oldpeak\"])\n",
    "ax4.set_title(\"oldpeak\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.1.1 Analysis Outputs(12) \n",
    "Age Variable\n",
    "We can see the exact image we want in this variable. There are no outliers. And the box stands at the midpoint of the whiskers.\n",
    "Trtbps Variable\n",
    "There are more outliers in the \"trtbps\" variable than the others.\n",
    "In terms of distribution, an image is very prone to normal distribution. The box appears near the middle of the two whiskers.\n",
    "Thalach Variable\n",
    "There are very few outliers. There is one outlier in part under the mustache.\n",
    "There is some closeness to the normal distribution in terms of distribution, but the box is slightly shifted upwards. It means that our data is somewhat concentrated on the right.\n",
    "In summary, we can say that there is a slight left skew.\n",
    "Oldpeak Variable\n",
    "There is no such thing as a lower whisker in the Oldpeak variable. The bottom of the box is at the level of the lower mustache. It means that the values are concentrated on the left side. So we can say that there is a queue to the right. So there is a right skew.\n",
    "If we examine the mustache at the top, we can see the outliers above the mustache.\n",
    "\n",
    "5.2.2 Dealing with outliers \n",
    "\n",
    "5.2.2.1 Trtbps Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats.mstats import winsorize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_trtbps = zscore(df[\"trtbps\"])\n",
    "for threshold in range(1, 4):\n",
    "    print(\"Threshold Value: {}\".format(threshold))\n",
    "    print(\"Number of Outliers: {}\".format(len(np.where(z_scores_trtbps > threshold)[0])))\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[z_scores_trtbps > 2][[\"trtbps\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"trtbps\"] < 170].trtbps.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorize_percentile_trtbps = (stats.percentileofscore(df[\"trtbps\"], 165)) / 100\n",
    "print(winsorize_percentile_trtbps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - winsorize_percentile_trtbps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trtbps_winsorize = winsorize(df.trtbps, (0, (1 - winsorize_percentile_trtbps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(trtbps_winsorize)\n",
    "plt.xlabel(\"trtbps_winsorize\", color = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"trtbps_winsorize\"] = trtbps_winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.2.2 Thalach Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(df, var):\n",
    "    q1 = np.quantile(df[var], 0.25)\n",
    "    q3 = np.quantile(df[var], 0.75)\n",
    "    diff = q3 - q1\n",
    "    lower_v = q1 - (1.5 * diff)\n",
    "    upper_v = q3 + (1.5 * diff)\n",
    "    return df[(df[var] < lower_v) | (df[var] > upper_v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thalach_out = iqr(df, \"thalach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thalach_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([272], axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"thalach\"][270:275]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df[\"thalach\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2.2.3 Oldpeak Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(df, var):\n",
    "    q1 = np.quantile(df[var], 0.25)\n",
    "    q3 = np.quantile(df[var], 0.75)\n",
    "    diff = q3 - q1\n",
    "    lower_v = q1 - (1.5 * diff)\n",
    "    upper_v = q3 + (1.5 * diff)\n",
    "    return df[(df[var] < lower_v) | (df[var] > upper_v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr(df, \"oldpeak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"oldpeak\"] < 4.2].oldpeak.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsorize_percentile_oldpeak = (stats.percentileofscore(df[\"oldpeak\"], 4)) / 100\n",
    "print(winsorize_percentile_oldpeak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldpeak_winsorize = winsorize(df.oldpeak, (0, (1 - winsorize_percentile_oldpeak)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(oldpeak_winsorize)\n",
    "plt.xlabel(\"oldpeak_winsorize\", color = \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"oldpeak_winsorize\"] = oldpeak_winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"trtbps\", \"oldpeak\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 Determining Distributions of Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize = (20, 6))\n",
    "\n",
    "ax1.hist(df[\"age\"])\n",
    "ax1.set_title(\"age\")\n",
    "\n",
    "ax2.hist(df[\"trtbps_winsorize\"])\n",
    "ax2.set_title(\"trtbps_winsorize\")\n",
    "\n",
    "ax3.hist(df[\"thalach\"])\n",
    "ax3.set_title(\"thalach\")\n",
    "\n",
    "ax4.hist(df[\"oldpeak_winsorize\"])\n",
    "ax4.set_title(\"oldpeak_winsorize\")\n",
    "\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"age\", \"trtbps_winsorize\", \"thalach\", \"oldpeak_winsorize\"]].agg([\"skew\"]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4 Transformation Operations on Unsymmetrical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"oldpeak_winsorize_log\"] = np.log(df[\"oldpeak_winsorize\"])\n",
    "df[\"oldpeak_winsorize_sqrt\"] = np.sqrt(df[\"oldpeak_winsorize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"oldpeak_winsorize\", \"oldpeak_winsorize_log\", \"oldpeak_winsorize_sqrt\"]].agg([\"skew\"]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"oldpeak_winsorize\", \"oldpeak_winsorize_log\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.5 Applying One Hot Encoding Method to Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_var.remove(\"fbs\")\n",
    "categoric_var.remove(\"rest_ecg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = pd.get_dummies(df_copy, columns = categoric_var[:-1], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6 Feature Scaling with the RobustScaler Method for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_numeric_var = [\"age\", \"thalach\", \"trtbps_winsorize\", \"oldpeak_winsorize_sqrt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robus_scaler = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[new_numeric_var] = robust_scaler.fit_transform(df_copy[new_numeric_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.7 Separating Data into Test and Training Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_copy.drop([\"target\"], axis = 1)\n",
    "y = df_copy[[\"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train: {X_train.shape[0]}\")\n",
    "print(f\"X_test: {X_test.shape[0]}\")\n",
    "print(f\"y_train: {y_train.shape[0]}\")\n",
    "print(f\"y_test: {y_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Modelling \n",
    "\n",
    "6.1 Logistic Regression Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.1 Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(log_reg, X_test, y_test, cv = 10)\n",
    "print(\"Cross-Validation Accuracy Scores\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.2 Roc Curve and Area Under Curve(AUC) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(log_reg, X_test, y_test, name = \"Logistic Regression\")\n",
    "plt.title(\"Logistic Regression Roc Curve And AUC\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1.3 Hyperparameter Optimization(with GridSearchCV) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_new = LogisticRegression()\n",
    "log_reg_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"penalty\":[\"l1\",\"l2\"], \"solver\" : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_grid = GridSearchCV(log_reg_new, param_grid = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters: \", log_reg_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_new2 = LogisticRegression(penalty = \"l1\", solver = \"saga\")\n",
    "log_reg_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_new2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg_new2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The test accuracy score of Logistic Regression After hyper-parameter tuning is: {}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(log_reg_new2, X_test, y_test, name = \"Logistic Regression GridSearchCV\")\n",
    "plt.title(\"Logistic Regression GridSearchCV Roc Curve And AUC\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Decision Tree Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree = DecisionTreeClassifier(random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dec_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dec_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(dec_tree, X_test, y_test, cv = 10)\n",
    "print(\"Cross-Validation Accuracy Scores\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(dec_tree, X_test, y_test, name = \"Decision Tree\")\n",
    "plt.title(\"Decision Tree Roc Curve And AUC\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.3 Support Vector Machine Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC(random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The test accuracy score of SVM is:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svc_model, X_test, y_test, cv = 10)\n",
    "print(\"Cross-Validation Accuracy Scores\", scores.mean())plot_roc_curve(svc_model, X_test, y_test, name = \"Support Vector Machine\")\n",
    "plt.title(\"Support Vector Machine Roc Curve And AUC\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4 Random Forest Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The test accuracy score of Random Forest is\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(random_forest, X_test, y_test, cv = 10)\n",
    "print(\"Cross-Validation Accuracy Scores\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(random_forest, X_test, y_test, name = \"Random Forest\")\n",
    "plt.title(\"Random Forest Roc Curve And AUC\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.4.1 Hyperparameter Optimization(with GridSearchCV) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_new = RandomForestClassifier(random_state = 5)\n",
    "random_forest_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_estimators\" : [50, 100, 150, 200], \n",
    "              \"criterion\" : [\"gini\", \"entropy\"], \n",
    "              'max_features': ['auto', 'sqrt', 'log2'], \n",
    "              'bootstrap': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_grid = GridSearchCV(random_forest_new, param_grid = parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", random_forest_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_new2 = RandomForestClassifier(bootstrap = True, criterion = \"entropy\", max_features = \"auto\", n_estimators = 200, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_new2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_forest_new2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The test accuracy score of Random Forest after hyper-parameter tuning is:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(random_forest_new2, X_test, y_test, name = \"Random Forest\")\n",
    "plt.title(\"Random Forest Roc Curve And AUC\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Project Conclusion \n",
    "The activities we carried out within the scope of the project are as follows:\n",
    "\n",
    "Within the scope of the project, we first made the data set ready for Exploratory Data Analysis(EDA)\n",
    "We performed Exploratory Data Analysis(EDA).\n",
    "We analyzed numerical and categorical variables within the scope of univariate analysis by using Distplot and Pie Chart graphics.\n",
    "Within the scope of bivariate analysis, we analyzed the variables among each other using FacetGrid, Count Plot, Pair Plot, Swarm plot, Box plot, and Heatmap graphics.\n",
    "We made the data set ready for the model. In this context, we struggled with missing and outlier values.\n",
    "We used four different algorithms in the model phase.\n",
    "We got 87% accuracy and 88% AUC with the Logistic Regression model.\n",
    "We got 83% accuracy and 85% AUC with the Decision Tree Model.\n",
    "We got 83% accuracy and 89% AUC with the Support Vector Classifier Model.\n",
    "And we got 90.3% accuracy and 93% AUC with the Random Forest Classifier Model.\n",
    "When all these model outputs are evaluated, we prefer the model we created with the Random Forest Algorithm, which gives the best results."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
